{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 179ms/step - accuracy: 0.4038 - loss: 1.0898 - val_accuracy: 0.5200 - val_loss: 0.9014\n",
      "Epoch 2/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 123ms/step - accuracy: 0.6291 - loss: 0.8318 - val_accuracy: 0.6350 - val_loss: 0.5546\n",
      "Epoch 3/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 125ms/step - accuracy: 0.6469 - loss: 0.5819 - val_accuracy: 0.6600 - val_loss: 0.4984\n",
      "Epoch 4/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 121ms/step - accuracy: 0.6977 - loss: 0.4918 - val_accuracy: 0.8400 - val_loss: 0.4216\n",
      "Epoch 5/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 129ms/step - accuracy: 0.7619 - loss: 0.4507 - val_accuracy: 0.8450 - val_loss: 0.3137\n",
      "Epoch 6/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 121ms/step - accuracy: 0.8056 - loss: 0.3765 - val_accuracy: 0.9950 - val_loss: 0.1044\n",
      "Epoch 7/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - accuracy: 0.9119 - loss: 0.2509 - val_accuracy: 0.9850 - val_loss: 0.0545\n",
      "Epoch 8/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 135ms/step - accuracy: 0.9193 - loss: 0.2318 - val_accuracy: 1.0000 - val_loss: 0.0260\n",
      "Epoch 9/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 126ms/step - accuracy: 0.9375 - loss: 0.1760 - val_accuracy: 1.0000 - val_loss: 0.0081\n",
      "Epoch 10/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - accuracy: 0.9841 - loss: 0.0671 - val_accuracy: 1.0000 - val_loss: 0.0039\n",
      "Epoch 11/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 137ms/step - accuracy: 0.9705 - loss: 0.1052 - val_accuracy: 1.0000 - val_loss: 0.0041\n",
      "Epoch 12/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 137ms/step - accuracy: 0.9890 - loss: 0.0568 - val_accuracy: 1.0000 - val_loss: 0.0020\n",
      "Epoch 13/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 122ms/step - accuracy: 0.9933 - loss: 0.0376 - val_accuracy: 1.0000 - val_loss: 0.0013\n",
      "Epoch 14/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 136ms/step - accuracy: 0.9839 - loss: 0.0620 - val_accuracy: 1.0000 - val_loss: 0.0013\n",
      "Epoch 15/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 124ms/step - accuracy: 0.9863 - loss: 0.0475 - val_accuracy: 1.0000 - val_loss: 9.4589e-04\n",
      "Epoch 16/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 121ms/step - accuracy: 0.9867 - loss: 0.0357 - val_accuracy: 1.0000 - val_loss: 6.2490e-04\n",
      "Epoch 17/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - accuracy: 0.9889 - loss: 0.0474 - val_accuracy: 1.0000 - val_loss: 4.3661e-04\n",
      "Epoch 18/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - accuracy: 0.9948 - loss: 0.0186 - val_accuracy: 1.0000 - val_loss: 2.9111e-04\n",
      "Epoch 19/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 121ms/step - accuracy: 0.9903 - loss: 0.0384 - val_accuracy: 1.0000 - val_loss: 2.8433e-04\n",
      "Epoch 20/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 134ms/step - accuracy: 0.9953 - loss: 0.0138 - val_accuracy: 1.0000 - val_loss: 1.5971e-04\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 1.5429e-04\n",
      " Accuracy with GloVe: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, SpatialDropout1D, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ‚úÖ Load dataset\n",
    "file_path = \"enhanced_event_reviews.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# ‚úÖ Preprocessing Function\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # Remove URLs\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove non-alphabetic chars\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "        return text\n",
    "    return \"\"\n",
    "\n",
    "# ‚úÖ Apply Preprocessing\n",
    "df['clean_review'] = df['review'].apply(clean_text)\n",
    "\n",
    "# ‚úÖ Map sentiment labels to integers\n",
    "sentiment_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n",
    "df['sentiment'] = df['sentiment'].map(sentiment_map)\n",
    "\n",
    "# ‚úÖ Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['clean_review'], df['sentiment'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ‚úÖ Tokenization and Padding\n",
    "MAX_NB_WORDS = 5000\n",
    "MAX_SEQUENCE_LENGTH = 150\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# ‚úÖ Load GloVe Embeddings\n",
    "glove_file = \"glove.6B.100d.txt\"\n",
    "embedding_index = {}\n",
    "\n",
    "with open(glove_file, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coefs\n",
    "\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(MAX_NB_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))  # 3 classes\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# ‚úÖ Model Training\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_pad, y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_pad, y_test),\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test_pad, y_test)\n",
    "print(f\" Accuracy with GloVe: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 110ms/step\n",
      "\n",
      "üî• Test Set Predictions:\n",
      "Review: it was just an ordinary experience the experience was frustrating due to poor organization\n",
      "Actual: neutral\n",
      "Predicted: neutral\n",
      "--------------------------------------------------\n",
      "Review: the event was poorly organized and chaotic the logistics were a mess which made the experience unpleasant\n",
      "Actual: negative\n",
      "Predicted: negative\n",
      "--------------------------------------------------\n",
      "Review: the decor was simple and clean nothing fancy i wouldnt recommend attending because of the lack of proper planning\n",
      "Actual: neutral\n",
      "Predicted: neutral\n",
      "--------------------------------------------------\n",
      "Review: great networking opportunities and friendly staff the entertainment and speakers were topnotch making the event worthwhile\n",
      "Actual: positive\n",
      "Predicted: positive\n",
      "--------------------------------------------------\n",
      "Review: the app for the event kept crashing the experience was frustrating due to poor organization\n",
      "Actual: negative\n",
      "Predicted: negative\n",
      "--------------------------------------------------\n",
      "Review: some sessions were good others were boring i wouldnt recommend attending because of the lack of proper planning\n",
      "Actual: neutral\n",
      "Predicted: neutral\n",
      "--------------------------------------------------\n",
      "Review: the event was absolutely fantastic the staff were friendly and everything was wellcoordinated\n",
      "Actual: positive\n",
      "Predicted: positive\n",
      "--------------------------------------------------\n",
      "Review: delayed sessions frustrated everyone the experience was frustrating due to poor organization\n",
      "Actual: negative\n",
      "Predicted: negative\n",
      "--------------------------------------------------\n",
      "Review: some sessions were good others were boring the experience was frustrating due to poor organization\n",
      "Actual: neutral\n",
      "Predicted: neutral\n",
      "--------------------------------------------------\n",
      "Review: everything was wellorganized and smooth the staff were friendly and everything was wellcoordinated\n",
      "Actual: positive\n",
      "Predicted: positive\n",
      "--------------------------------------------------\n",
      "\n",
      "üî• Test Accuracy: 1.0000\n",
      "\n",
      "üî• Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        61\n",
      "     neutral       1.00      1.00      1.00        80\n",
      "    positive       1.00      1.00      1.00        59\n",
      "\n",
      "    accuracy                           1.00       200\n",
      "   macro avg       1.00      1.00      1.00       200\n",
      "weighted avg       1.00      1.00      1.00       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ‚úÖ Make Predictions\n",
    "y_pred_probs = model.predict(X_test_pad)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)  # Get the class with highest probability\n",
    "\n",
    "# ‚úÖ Map Sentiment Labels\n",
    "reverse_sentiment_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "\n",
    "# ‚úÖ Display Predictions with Actual Labels\n",
    "print(\"\\nüî• Test Set Predictions:\")\n",
    "for i in range(10):  # Display first 10 predictions\n",
    "    print(f\"Review: {X_test.iloc[i]}\")\n",
    "    print(f\"Actual: {reverse_sentiment_map[y_test.iloc[i]]}\")\n",
    "    print(f\"Predicted: {reverse_sentiment_map[y_pred[i]]}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# ‚úÖ Calculate Accuracy on Test Set\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nüî• Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# ‚úÖ Classification Report\n",
    "print(\"\\nüî• Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['negative', 'neutral', 'positive']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\n",
      " New Review Predictions:\n",
      "Review: The waiter molested me in the washroom\n",
      "Predicted Sentiment: negative\n",
      "--------------------------------------------------\n",
      "Review: The waiter kissed me passionately in the washroom\n",
      "Predicted Sentiment: negative\n",
      "--------------------------------------------------\n",
      "Review: The experience was vey good\n",
      "Predicted Sentiment: positive\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ New Reviews\n",
    "new_reviews = [\n",
    "    \"The waiter molested me in the washroom\",\n",
    "    \"The waiter kissed me passionately in the washroom\",\n",
    "    \"The experience was vey good\"\n",
    "]\n",
    "\n",
    "# ‚úÖ Preprocess New Reviews\n",
    "new_reviews_clean = [clean_text(review) for review in new_reviews]\n",
    "\n",
    "# ‚úÖ Tokenize and Pad\n",
    "new_reviews_seq = tokenizer.texts_to_sequences(new_reviews_clean)\n",
    "new_reviews_pad = pad_sequences(new_reviews_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# ‚úÖ Make Predictions\n",
    "new_preds_probs = model.predict(new_reviews_pad)\n",
    "new_preds = np.argmax(new_preds_probs, axis=1)\n",
    "\n",
    "# ‚úÖ Map Sentiment Labels\n",
    "print(\"\\n New Review Predictions:\")\n",
    "for i, review in enumerate(new_reviews):\n",
    "    sentiment = reverse_sentiment_map[new_preds[i]]\n",
    "    print(f\"Review: {review}\")\n",
    "    print(f\"Predicted Sentiment: {sentiment}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'was': 2,\n",
       " 'and': 3,\n",
       " 'but': 4,\n",
       " 'event': 5,\n",
       " 'not': 6,\n",
       " 'very': 7,\n",
       " 'sessions': 8,\n",
       " 'experience': 9,\n",
       " 'were': 10,\n",
       " 'no': 11,\n",
       " 'food': 12,\n",
       " 'a': 13,\n",
       " 'process': 14,\n",
       " 'lacked': 15,\n",
       " 'quality': 16,\n",
       " 'great': 17,\n",
       " 'speakers': 18,\n",
       " 'waiting': 19,\n",
       " 'boring': 20,\n",
       " 'nothing': 21,\n",
       " 'crowd': 22,\n",
       " 'proper': 23,\n",
       " 'seating': 24,\n",
       " 'arrangement': 25,\n",
       " 'uncomfortable': 26,\n",
       " 'absolutely': 27,\n",
       " 'fantastic': 28,\n",
       " 'registration': 29,\n",
       " 'mess': 30,\n",
       " 'app': 31,\n",
       " 'for': 32,\n",
       " 'kept': 33,\n",
       " 'crashing': 34,\n",
       " 'highquality': 35,\n",
       " 'workshops': 36,\n",
       " 'interactive': 37,\n",
       " 'excellent': 38,\n",
       " 'beverage': 39,\n",
       " 'service': 40,\n",
       " 'subpar': 41,\n",
       " 'disappointing': 42,\n",
       " 'panel': 43,\n",
       " 'discussion': 44,\n",
       " 'okay': 45,\n",
       " 'memorable': 46,\n",
       " 'decent': 47,\n",
       " 'engagement': 48,\n",
       " 'average': 49,\n",
       " 'bad': 50,\n",
       " 'keynote': 51,\n",
       " 'speech': 52,\n",
       " 'inspiring': 53,\n",
       " 'perfect': 54,\n",
       " 'venue': 55,\n",
       " 'with': 56,\n",
       " 'ample': 57,\n",
       " 'space': 58,\n",
       " 'facilities': 59,\n",
       " 'uninspiring': 60,\n",
       " 'schedule': 61,\n",
       " 'punctual': 62,\n",
       " 'energy': 63,\n",
       " 'session': 64,\n",
       " 'informative': 65,\n",
       " 'bit': 66,\n",
       " 'slow': 67,\n",
       " 'seamless': 68,\n",
       " 'checkin': 69,\n",
       " 'time': 70,\n",
       " 'networking': 71,\n",
       " 'opportunities': 72,\n",
       " 'friendly': 73,\n",
       " 'staff': 74,\n",
       " 'everything': 75,\n",
       " 'wellorganized': 76,\n",
       " 'smooth': 77,\n",
       " 'audio': 78,\n",
       " 'terrible': 79,\n",
       " 'unclear': 80,\n",
       " 'fine': 81,\n",
       " 'extraordinary': 82,\n",
       " 'location': 83,\n",
       " 'convenient': 84,\n",
       " 'parking': 85,\n",
       " 'limited': 86,\n",
       " 'delayed': 87,\n",
       " 'frustrated': 88,\n",
       " 'everyone': 89,\n",
       " 'long': 90,\n",
       " 'times': 91,\n",
       " 'ruined': 92,\n",
       " 'loved': 93,\n",
       " 'presentation': 94,\n",
       " 'insights': 95,\n",
       " 'live': 96,\n",
       " 'music': 97,\n",
       " 'performance': 98,\n",
       " 'mesmerizing': 99,\n",
       " 'decor': 100,\n",
       " 'simple': 101,\n",
       " 'clean': 102,\n",
       " 'fancy': 103,\n",
       " 'some': 104,\n",
       " 'good': 105,\n",
       " 'others': 106,\n",
       " 'poorly': 107,\n",
       " 'organized': 108,\n",
       " 'chaotic': 109,\n",
       " 'it': 110,\n",
       " 'just': 111,\n",
       " 'an': 112,\n",
       " 'ordinary': 113,\n",
       " 'management': 114,\n",
       " 'poor': 115}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('sentiment_model_enhanced.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save the tokenizer\n",
    "with open('tokenizer_enhanced.pkl', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
